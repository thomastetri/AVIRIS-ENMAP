{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048babf8-3272-4877-a9b3-38b95f419805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pystac_client import Client\n",
    "from shapely.geometry import shape, box, mapping\n",
    "\n",
    "\n",
    "def get_aviris_data(collection):\n",
    "    \"\"\"\n",
    "    Get all AVIRIS data from a specific collection\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    collection : str\n",
    "        The collection to query from cmr.earthdata.nasa.gov\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        A list of granules / objects\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://cmr.earthdata.nasa.gov/search/granules.json\"\n",
    "    params = {\n",
    "        \"collection_concept_id\": collection,  # \"C2659129205-ORNL_CLOUD\",\n",
    "        \"page_size\": 1000,\n",
    "        \"page_num\": 1,\n",
    "        \"sort_key\": \"-start_date\",\n",
    "        \"temporal\": \"2022-07-01T00:00:00.000Z,\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Raise an error if something went wrong\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse JSON into a Python dictionary\n",
    "    data = response.json()\n",
    "\n",
    "    # Example: print how many granules you got\n",
    "    granules = len(data.get(\"feed\", {}).get(\"entry\", []))\n",
    "    print(f\"Got {granules} granules, aka {granules // 2} images (.bin + .hdr files)\")\n",
    "\n",
    "    # Access the whole dictionary if you want\n",
    "    granules = data[\"feed\"][\"entry\"]\n",
    "    granules = sorted(granules, key=lambda x: x[\"title\"])\n",
    "\n",
    "    return granules\n",
    "\n",
    "\n",
    "def get_enmap_images(aoi, start_date, end_date, cloud_cover_max=50):\n",
    "    \"\"\"\n",
    "    Query and plot EnMAP L2A images from the DLR STAC API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    aoi : dict or list[float]\n",
    "        Area of interest, either a GeoJSON Polygon or bounding box [minx, miny, maxx, maxy].\n",
    "    start_date : str\n",
    "        Start of date range, e.g. \"2025-01-01\".\n",
    "    end_date : str\n",
    "        End of date range, e.g. \"2025-11-01\".\n",
    "    cloud_cover_max : float\n",
    "        Maximum allowed cloud coverage (%). Default 50.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.GeoDataFrame\n",
    "        containing all EnMAP images that match the filters\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert bounding box to GeoJSON polygon if needed\n",
    "    if isinstance(aoi, list) and len(aoi) == 4:\n",
    "        geom = box(*aoi)\n",
    "    else:\n",
    "        geom = shape(aoi)\n",
    "\n",
    "    # STAC API endpoint\n",
    "    stac_url = \"https://geoservice.dlr.de/eoc/ogc/stac/v1\"\n",
    "\n",
    "    # Open STAC client\n",
    "    client = Client.open(stac_url)\n",
    "\n",
    "    # Search collection\n",
    "    search = client.search(\n",
    "        collections=[\"ENMAP_HSI_L2A\"],\n",
    "        intersects=mapping(geom),\n",
    "        datetime=f\"{start_date}/{end_date}\",\n",
    "        query={\"eo:cloud_cover\": {\"lt\": cloud_cover_max}},\n",
    "        limit=200,\n",
    "    )\n",
    "\n",
    "    # Convert results to GeoDataFrame\n",
    "    items = list(search.get_items())\n",
    "    if not items:\n",
    "        print(\"No matching scenes found.\")\n",
    "        return None\n",
    "    print(f\"Found {len(items)} scenes\")\n",
    "\n",
    "    # Extract just geometries + key properties for plotting\n",
    "    plot_features = []\n",
    "    for item in items:\n",
    "        props = item.properties.copy()\n",
    "        props[\"id\"] = item.id\n",
    "        props[\"cloud_cover\"] = props.get(\"eo:cloud_cover\")\n",
    "        props[\"href_self\"] = item.get_self_href()\n",
    "        props[\"asset_keys\"] = list(item.assets.keys())\n",
    "        props[\"image\"] = item.assets.get(\"image\")\n",
    "        plot_features.append(\n",
    "            {\"type\": \"Feature\", \"geometry\": item.geometry, \"properties\": props}\n",
    "        )\n",
    "\n",
    "    gdf = gpd.GeoDataFrame.from_features(plot_features, crs=\"EPSG:4326\")\n",
    "    gdf = gdf[gdf[\"eo:cloud_cover\"].astype(int) < cloud_cover_max]\n",
    "    print(f\"{len(gdf)} scenes have less than {cloud_cover_max} cloud coverage ...\")\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc176981-d9b7-4347-af54-8e0a46e9448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aviris_granules = get_aviris_data(\"C2659129205-ORNL_CLOUD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18bb8c9-5a79-4218-b564-24db2b47cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_below_45_deg(raw_geom):\n",
    "    # split into floats\n",
    "    vals = list(map(float, raw_geom.split()))\n",
    "    \n",
    "    # group into coordinate pairs (lat, lon → lon, lat)\n",
    "    coords = [[vals[i+1], vals[i]] for i in range(0, len(vals), 2)]  # lat, lon pairs\n",
    "\n",
    "    return coords[0][1] < 45\n",
    "\n",
    "aviris_granules_filtered = list(filter(lambda x: is_below_45_deg(x['polygons'][0][0]), aviris_granules))\n",
    "print(f\"Selecting only those with latitude < 45: length now: {len(aviris_granules_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0819b37-db46-4d6c-9453-5ba4dd4c36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_enmap_file(url, fname):           \n",
    "    print(f\"⬇️ Downloading {fname} from {url} ...\")\n",
    "    SESSION = \"_q9f8voUZmHEwCNnFUZDxQ|1762983559|FKCFV_PU6EWqo7R3I3tLIOPtCGqJ3kT46cwm0JnUBr0cHksiVvlqc9m-zcj7gYTzCF6SPTnUOfr1XDrGvE3A_JlURmzcsBKv_2BaSK-N-u-MW9Tp1gk4haPKXiSutL2tfHRiMoFFNUnEZJHb57ErFgQoQLIV4heNx1oDO6MBdFSQpOK2A3DjFImH68bMAZ2mN5T2IrP3YsE_N1OXXZl-9iatnEdFUOfzVTQPtmDQUnQ|zf1q0sSHaG_IKzSh9mT-uDEicI4\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:144.0) Gecko/20100101 Firefox/144.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        f\"Cookie\": \"session={SESSION}\",\n",
    "    }\n",
    "    \n",
    "    # Stream to avoid loading large file in memory\n",
    "    with requests.get(url, headers=headers, stream=True, timeout=600) as r:\n",
    "        if r.status_code == 200 and \"text/html\" not in r.headers.get(\"Content-Type\", \"\"):\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=16384):\n",
    "                    f.write(chunk)\n",
    "            print(\"✅ Download complete\")\n",
    "        else:\n",
    "            print(\"⚠️ Something went wrong:\", r.status_code)\n",
    "            print(\"Returned URL:\", r.url)\n",
    "            print(\"Response headers:\", r.headers)\n",
    "\n",
    "\n",
    "def download_aviris_file(url, fname):           \n",
    "    print(f\"⬇️ Downloading {fname} from {url} ...\")\n",
    "    ASF_URS = \"\"\n",
    "    ACCESS_TOKEN = \"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:144.0) Gecko/20100101 Firefox/144.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": (\n",
    "            \"urs_user_already_logged=yes; \"\n",
    "            \"urs_guid_ops=03a1a0ea-4bc0-4490-b199-6b26c40bc995; \"\n",
    "            f\"asf-urs={ASF_URS}; \"\n",
    "            f\"accessToken={ACCESS_TOKEN}\"\n",
    "        ),\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Priority\": \"u=0, i\",\n",
    "        \"TE\": \"trailers\"\n",
    "    }\n",
    "    \n",
    "    # Stream to avoid loading large file in memory\n",
    "    with requests.get(url, headers=headers, stream=True, timeout=600) as r:\n",
    "        if r.status_code == 200 and \"text/html\" not in r.headers.get(\"Content-Type\", \"\"):\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=16384):\n",
    "                    f.write(chunk)\n",
    "            print(\"✅ Download complete\")\n",
    "        else:\n",
    "            print(\"⚠️ Something went wrong:\", r.status_code)\n",
    "            print(\"Returned URL:\", r.url)\n",
    "            print(\"Response headers:\", r.headers)\n",
    "\n",
    "\n",
    "BASE_DOWNLOAD_DIR = \".\"\n",
    "\n",
    "for g in aviris_granules_filtered[::2]:\n",
    "    print(g['title'])\n",
    "    ########### First, do geometry ##########\n",
    "    \n",
    "    raw_geom = g['polygons'][0][0]\n",
    "    # g['polygons'] is like [['66.4241714 -147.238739 66.4497986 -147.2604065 66.3321457 -147.9172363 66.3065262 -147.8955231 66.4241714 -147.238739']]\n",
    "    \n",
    "    # split into floats\n",
    "    vals = list(map(float, raw_geom.split()))\n",
    "    \n",
    "    # group into coordinate pairs (lat, lon → lon, lat)\n",
    "    coords = [[vals[i+1], vals[i]] for i in range(0, len(vals), 2)]  # lat, lon pairs\n",
    "    print(coords)\n",
    "    \n",
    "    # turn into a GeoJSON polygon\n",
    "    aoi = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [coords]\n",
    "    }\n",
    "\n",
    "    ########### Then, -1 to +1 month ##########\n",
    "    time_start_str = g['time_start']\n",
    "    \n",
    "    # Convert string to datetime object\n",
    "    time_start = datetime.strptime(time_start_str, \"%Y-%m-%dT%H:%M:%S.%fZ\").date()\n",
    "    \n",
    "    # Compute start and end dates ±1 month\n",
    "    start_date = time_start - relativedelta(months=1)\n",
    "    end_date   = time_start + relativedelta(months=1)\n",
    "    \n",
    "    gdf = get_enmap_images(aoi, start_date, end_date, cloud_cover_max=30)\n",
    "    if gdf is not None:\n",
    "        # First, drop duplicates with old version number \n",
    "        \n",
    "        # Make sure datetime columns are parsed\n",
    "        gdf['start_datetime'] = pd.to_datetime(gdf['start_datetime'])\n",
    "        gdf['version'] = gdf['version'].astype(str)\n",
    "        \n",
    "        # Sort by start_datetime (ascending) and version (descending)\n",
    "        # Assumes version strings like \"01.05.02\" compare correctly lexicographically\n",
    "        gdf_sorted = gdf.sort_values(\n",
    "            by=['start_datetime', 'version'],\n",
    "            ascending=[True, False]\n",
    "        )\n",
    "        \n",
    "        # Drop the duplicates with old version now\n",
    "        gdf_unique = gdf_sorted.drop_duplicates(subset='start_datetime', keep='first')\n",
    "\n",
    "        # Let's download stuff\n",
    "        # AVIRIS first\n",
    "        # Links are both https and S3\n",
    "        img_bin_links = sorted([link['href'] for link in g['links'] if link['href'].endswith('_img.bin')])\n",
    "        # sorted, https:// comes before s3://\n",
    "        download_aviris_file(img_bin_links[0], os.path.join(BASE_DOWNLOAD_DIR, g['title']))\n",
    "        \n",
    "        # Download all EnMAPimages from gdf_unique \n",
    "        for _, row in gdf_unique.iterrows():\n",
    "            url = row[\"image\"].href\n",
    "            download_dir = os.path.join(BASE_DOWNLOAD_DIR, os.path.splitext(g['title'])[0])\n",
    "            os.path.makedirs(download_dir, exist_ok=True)\n",
    "            fname = os.path.join(download_dir, f\"{row.id}.tif\")\n",
    "            if not os.path.exists(fname):\n",
    "                download_enmap_file(url, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc1e47-dd35-42b8-bcc1-b13914c241da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
